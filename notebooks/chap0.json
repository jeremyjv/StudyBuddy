{
    "content": "Chapter 0\nPrologue\nLook around you. Computers and networks are everywhere, enabling an intricate web of com-\nplex human activities: education, commerce, entertainment, research, manufacturing, health\nmanagement, human communication, even war. Of the two main technological underpinnings\nof this amazing proliferation, one is obvious: the breathtaking pace with which advances in\nmicroelectronics and chip design have been bringing us faster and faster hardware.\nThis book tells the story of the other intellectual enterprise that is crucially fueling the\ncomputer revolution: ef\ufb01cient algorithms. It is a fascinating story.\nGather \u2019round and listen close.\n0.1\nBooks and algorithms\nTwo ideas changed the world. In 1448 in the German city of Mainz a goldsmith named Jo-\nhann Gutenberg discovered a way to print books by putting together movable metallic pieces.\nLiteracy spread, the Dark Ages ended, the human intellect was liberated, science and tech-\nnology triumphed, the Industrial Revolution happened. Many historians say we owe all this\nto typography. Imagine a world in which only an elite could read these lines! But others insist\nthat the key development was not typography, but algorithms.\nToday we are so used to writing numbers in decimal, that it is easy to forget that Guten-\nberg would write the number 1448 as MCDXLVIII. How do you add two Roman numerals?\nWhat is MCDXLVIII + DCCCXII? (And just try to think about multiplying them.) Even a\nclever man like Gutenberg probably only knew how to add and subtract small numbers using\nhis \ufb01ngers; for anything more complicated he had to consult an abacus specialist.\nThe decimal system, invented in India around AD 600, was a revolution in quantitative\nreasoning: using only 10 symbols, even very large numbers could be written down compactly,\nand arithmetic could be done ef\ufb01ciently on them by following elementary steps. Nonetheless\nthese ideas took a long time to spread, hindered by traditional barriers of language, distance,\nand ignorance. The most in\ufb02uential medium of transmission turned out to be a textbook,\nwritten in Arabic in the ninth century by a man who lived in Baghdad. Al Khwarizmi laid\nout the basic methods for adding, multiplying, and dividing numbers\u2014even extracting square\nroots and calculating digits of \u03c0. These procedures were precise, unambiguous, mechanical,\n11\n12\nAlgorithms\nef\ufb01cient, correct\u2014in short, they were algorithms, a term coined to honor the wise man after\nthe decimal system was \ufb01nally adopted in Europe, many centuries later.\nSince then, this decimal positional system and its numerical algorithms have played an\nenormous role in Western civilization. They enabled science and technology; they acceler-\nated industry and commerce. And when, much later, the computer was \ufb01nally designed, it\nexplicitly embodied the positional system in its bits and words and arithmetic unit. Scien-\ntists everywhere then got busy developing more and more complex algorithms for all kinds of\nproblems and inventing novel applications\u2014ultimately changing the world.\n0.2\nEnter Fibonacci\nAl Khwarizmi\u2019s work could not have gained a foothold in the West were it not for the efforts of\none man: the 15th century Italian mathematician Leonardo Fibonacci, who saw the potential\nof the positional system and worked hard to develop it further and propagandize it.\nBut today Fibonacci is most widely known for his famous sequence of numbers\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, . . . ,\neach the sum of its two immediate predecessors. More formally, the Fibonacci numbers Fn are\ngenerated by the simple rule\nFn =\n\uf8f1\n\uf8f2\n\uf8f3\nFn\u22121 + Fn\u22122\nif n > 1\n1\nif n = 1\n0\nif n = 0 .\nNo other sequence of numbers has been studied as extensively, or applied to more \ufb01elds:\nbiology, demography, art, architecture, music, to name just a few. And, together with the\npowers of 2, it is computer science\u2019s favorite sequence.\nIn fact, the Fibonacci numbers grow almost as fast as the powers of 2: for example, F30 is\nover a million, and F100 is already 21 digits long! In general, Fn \u224820.694n (see Exercise 0.3).\nBut what is the precise value of F100, or of F200? Fibonacci himself would surely have\nwanted to know such things. To answer, we need an algorithm for computing the nth Fibonacci\nnumber.\nAn exponential algorithm\nOne idea is to slavishly implement the recursive de\ufb01nition of Fn. Here is the resulting algo-\nrithm, in the \u201cpseudocode\u201d notation used throughout this book:\nfunction fib1(n)\nif n = 0:\nreturn 0\nif n = 1:\nreturn 1\nreturn fib1(n \u22121) + fib1(n \u22122)\nWhenever we have an algorithm, there are three questions we always ask about it:\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n13\n1. Is it correct?\n2. How much time does it take, as a function of n?\n3. And can we do better?\nThe \ufb01rst question is moot here, as this algorithm is precisely Fibonacci\u2019s de\ufb01nition of Fn.\nBut the second demands an answer. Let T(n) be the number of computer steps needed to\ncompute fib1(n); what can we say about this function? For starters, if n is less than 2, the\nprocedure halts almost immediately, after just a couple of steps. Therefore,\nT(n) \u22642 for n \u22641.\nFor larger values of n, there are two recursive invocations of fib1, taking time T(n \u22121) and\nT(n\u22122), respectively, plus three computer steps (checks on the value of n and a \ufb01nal addition).\nTherefore,\nT(n) = T(n \u22121) + T(n \u22122) + 3 for n > 1.\nCompare this to the recurrence relation for Fn: we immediately see that T(n) \u2265Fn.\nThis is very bad news: the running time of the algorithm grows as fast as the Fibonacci\nnumbers! T(n) is exponential in n, which implies that the algorithm is impractically slow\nexcept for very small values of n.\nLet\u2019s be a little more concrete about just how bad exponential time is. To compute F200,\nthe fib1 algorithm executes T(200) \u2265F200 \u22652138 elementary computer steps. How long this\nactually takes depends, of course, on the computer used. At this time, the fastest computer\nin the world is the NEC Earth Simulator, which clocks 40 trillion steps per second. Even on\nthis machine, fib1(200) would take at least 292 seconds. This means that, if we start the\ncomputation today, it would still be going long after the sun turns into a red giant star.\nBut technology is rapidly improving\u2014computer speeds have been doubling roughly every\n18 months, a phenomenon sometimes called Moore\u2019s law. With this extraordinary growth,\nperhaps fib1 will run a lot faster on next year\u2019s machines. Let\u2019s see\u2014the running time of\nfib1(n) is proportional to 20.694n \u2248(1.6)n, so it takes 1.6 times longer to compute Fn+1 than\nFn. And under Moore\u2019s law, computers get roughly 1.6 times faster each year. So if we can\nreasonably compute F100 with this year\u2019s technology, then next year we will manage F101. And\nthe year after, F102. And so on: just one more Fibonacci number every year! Such is the curse\nof exponential time.\nIn short, our naive recursive algorithm is correct but hopelessly inef\ufb01cient. Can we do\nbetter?\nA polynomial algorithm\nLet\u2019s try to understand why fib1 is so slow. Figure 0.1 shows the cascade of recursive invo-\ncations triggered by a single call to fib1(n). Notice that many computations are repeated!\nA more sensible scheme would store the intermediate results\u2014the values F0, F1, . . . , Fn\u22121\u2014\nas soon as they become known.\n14\nAlgorithms\nFigure 0.1 The proliferation of recursive calls in fib1.\n\u0000\u0001\u0002\u0003\n\u0004\u0005\n\u0006\u0007\nFn\u22123\nFn\u22121\nFn\u22124\nFn\u22122\nFn\u22124\nFn\u22126\nFn\u22125\nFn\u22124\nFn\u22122\nFn\u22123\nFn\u22123\nFn\u22124\nFn\u22125\nFn\u22125\nFn\nfunction fib2(n)\nif n = 0 return 0\ncreate an array f[0 . . . n]\nf[0] = 0, f[1] = 1\nfor i = 2 . . . n:\nf[i] = f[i \u22121] + f[i \u22122]\nreturn f[n]\nAs with fib1, the correctness of this algorithm is self-evident because it directly uses the\nde\ufb01nition of Fn. How long does it take? The inner loop consists of a single computer step and\nis executed n \u22121 times. Therefore the number of computer steps used by fib2 is linear in n.\nFrom exponential we are down to polynomial, a huge breakthrough in running time. It is now\nperfectly reasonable to compute F200 or even F200,000.1\nAs we will see repeatedly throughout this book, the right algorithm makes all the differ-\nence.\nMore careful analysis\nIn our discussion so far, we have been counting the number of basic computer steps executed\nby each algorithm and thinking of these basic steps as taking a constant amount of time.\nThis is a very useful simpli\ufb01cation. After all, a processor\u2019s instruction set has a variety of\nbasic primitives\u2014branching, storing to memory, comparing numbers, simple arithmetic, and\n1To better appreciate the importance of this dichotomy between exponential and polynomial algorithms, the\nreader may want to peek ahead to the story of Sissa and Moore, in Chapter 8.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n15\nso on\u2014and rather than distinguishing between these elementary operations, it is far more\nconvenient to lump them together into one category.\nBut looking back at our treatment of Fibonacci algorithms, we have been too liberal with\nwhat we consider a basic step. It is reasonable to treat addition as a single computer step if\nsmall numbers are being added, 32-bit numbers say. But the nth Fibonacci number is about\n0.694n bits long, and this can far exceed 32 as n grows. Arithmetic operations on arbitrarily\nlarge numbers cannot possibly be performed in a single, constant-time step. We need to audit\nour earlier running time estimates and make them more honest.\nWe will see in Chapter 1 that the addition of two n-bit numbers takes time roughly propor-\ntional to n; this is not too hard to understand if you think back to the grade-school procedure\nfor addition, which works on one digit at a time. Thus fib1, which performs about Fn ad-\nditions, actually uses a number of basic steps roughly proportional to nFn. Likewise, the\nnumber of steps taken by fib2 is proportional to n2, still polynomial in n and therefore ex-\nponentially superior to fib1. This correction to the running time analysis does not diminish\nour breakthrough.\nBut can we do even better than fib2? Indeed we can: see Exercise 0.4.\n0.3\nBig-O notation\nWe\u2019ve just seen how sloppiness in the analysis of running times can lead to an unacceptable\nlevel of inaccuracy in the result. But the opposite danger is also present: it is possible to be\ntoo precise. An insightful analysis is based on the right simpli\ufb01cations.\nExpressing running time in terms of basic computer steps is already a simpli\ufb01cation. After\nall, the time taken by one such step depends crucially on the particular processor and even on\ndetails such as caching strategy (as a result of which the running time can differ subtly from\none execution to the next). Accounting for these architecture-speci\ufb01c minutiae is a nightmar-\nishly complex task and yields a result that does not generalize from one computer to the next.\nIt therefore makes more sense to seek an uncluttered, machine-independent characterization\nof an algorithm\u2019s ef\ufb01ciency. To this end, we will always express running time by counting the\nnumber of basic computer steps, as a function of the size of the input.\nAnd this simpli\ufb01cation leads to another. Instead of reporting that an algorithm takes, say,\n5n3 + 4n + 3 steps on an input of size n, it is much simpler to leave out lower-order terms such\nas 4n and 3 (which become insigni\ufb01cant as n grows), and even the detail of the coef\ufb01cient 5\nin the leading term (computers will be \ufb01ve times faster in a few years anyway), and just say\nthat the algorithm takes time O(n3) (pronounced \u201cbig oh of n3\u201d).\nIt is time to de\ufb01ne this notation precisely. In what follows, think of f(n) and g(n) as the\nrunning times of two algorithms on inputs of size n.\nLet f(n) and g(n) be functions from positive integers to positive reals. We say\nf = O(g) (which means that \u201cf grows no faster than g\u201d) if there is a constant c > 0\nsuch that f(n) \u2264c \u00b7 g(n).\nSaying f = O(g) is a very loose analog of \u201cf \u2264g.\u201d It differs from the usual notion of \u2264\nbecause of the constant c, so that for instance 10n = O(n). This constant also allows us to\n16\nAlgorithms\nFigure 0.2 Which running time is better?\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nn\n2n+20 \nn2 \ndisregard what happens for small values of n. For example, suppose we are choosing between\ntwo algorithms for a particular computational task. One takes f1(n) = n2 steps, while the\nother takes f2(n) = 2n + 20 steps (Figure 0.2). Which is better? Well, this depends on the\nvalue of n. For n \u22645, f1 is smaller; thereafter, f2 is the clear winner. In this case, f2 scales\nmuch better as n grows, and therefore it is superior.\nThis superiority is captured by the big-O notation: f2 = O(f1), because\nf2(n)\nf1(n) = 2n + 20\nn2\n\u226422\nfor all n; on the other hand, f1 \u0338= O(f2), since the ratio f1(n)/f2(n) = n2/(2n + 20) can get\narbitrarily large, and so no constant c will make the de\ufb01nition work.\nNow another algorithm comes along, one that uses f3(n) = n + 1 steps. Is this better\nthan f2? Certainly, but only by a constant factor. The discrepancy between f2 and f3 is tiny\ncompared to the huge gap between f1 and f2. In order to stay focused on the big picture, we\ntreat functions as equivalent if they differ only by multiplicative constants.\nReturning to the de\ufb01nition of big-O, we see that f2 = O(f3):\nf2(n)\nf3(n) = 2n + 20\nn + 1\n\u226420,\nand of course f3 = O(f2), this time with c = 1.\nJust as O(\u00b7) is an analog of \u2264, we can also de\ufb01ne analogs of \u2265and = as follows:\nf = \u2126(g) means g = O(f)\nf = \u0398(g) means f = O(g) and f = \u2126(g).\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n17\nIn the preceding example, f2 = \u0398(f3) and f1 = \u2126(f3).\nBig-O notation lets us focus on the big picture. When faced with a complicated function\nlike 3n2 + 4n + 5, we just replace it with O(f(n)), where f(n) is as simple as possible. In this\nparticular example we\u2019d use O(n2), because the quadratic portion of the sum dominates the\nrest. Here are some commonsense rules that help simplify functions by omitting dominated\nterms:\n1. Multiplicative constants can be omitted: 14n2 becomes n2.\n2. na dominates nb if a > b: for instance, n2 dominates n.\n3. Any exponential dominates any polynomial: 3n dominates n5 (it even dominates 2n).\n4. Likewise, any polynomial dominates any logarithm: n dominates (log n)3.\nThis also\nmeans, for example, that n2 dominates n log n.\nDon\u2019t misunderstand this cavalier attitude toward constants. Programmers and algorithm\ndevelopers are very interested in constants and would gladly stay up nights in order to make\nan algorithm run faster by a factor of 2. But understanding algorithms at the level of this\nbook would be impossible without the simplicity afforded by big-O notation.\n18\nAlgorithms\nExercises\n0.1. In each of the following situations, indicate whether f = O(g), or f = \u2126(g), or both (in which case\nf = \u0398(g)).\nf(n)\ng(n)\n(a)\nn \u2212100\nn \u2212200\n(b)\nn1/2\nn2/3\n(c)\n100n + log n\nn + (log n)2\n(d)\nn log n\n10n log 10n\n(e)\nlog 2n\nlog 3n\n(f)\n10 log n\nlog(n2)\n(g)\nn1.01\nn log2 n\n(h)\nn2/ log n\nn(log n)2\n(i)\nn0.1\n(log n)10\n(j)\n(log n)log n\nn/ log n\n(k)\n\u221an\n(log n)3\n(l)\nn1/2\n5log2 n\n(m)\nn2n\n3n\n(n)\n2n\n2n+1\n(o)\nn!\n2n\n(p)\n(log n)log n\n2(log2 n)2\n(q)\nPn\ni=1 ik\nnk+1\n0.2. Show that, if c is a positive real number, then g(n) = 1 + c + c2 + \u00b7 \u00b7 \u00b7 + cn is:\n(a) \u0398(1) if c < 1.\n(b) \u0398(n) if c = 1.\n(c) \u0398(cn) if c > 1.\nThe moral: in big-\u0398 terms, the sum of a geometric series is simply the \ufb01rst term if the series is\nstrictly decreasing, the last term if the series is strictly increasing, or the number of terms if the\nseries is unchanging.\n0.3. The Fibonacci numbers F0, F1, F2, . . . , are de\ufb01ned by the rule\nF0 = 0, F1 = 1, Fn = Fn\u22121 + Fn\u22122.\nIn this problem we will con\ufb01rm that this sequence grows exponentially fast and obtain some\nbounds on its growth.\n(a) Use induction to prove that Fn \u226520.5n for n \u22656.\n(b) Find a constant c < 1 such that Fn \u22642cn for all n \u22650. Show that your answer is correct.\n(c) What is the largest c you can \ufb01nd for which Fn = \u2126(2cn)?\n0.4. Is there a faster way to compute the nth Fibonacci number than by fib2 (page 13)? One idea\ninvolves matrices.\nWe start by writing the equations F1 = F1 and F2 = F0 + F1 in matrix notation:\n\u0012F1\nF2\n\u0013\n=\n\u00120\n1\n1\n1\n\u0013\n\u00b7\n\u0012F0\nF1\n\u0013\n.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n19\nSimilarly,\n\u0012F2\nF3\n\u0013\n=\n\u00120\n1\n1\n1\n\u0013\n\u00b7\n\u0012F1\nF2\n\u0013\n=\n\u00120\n1\n1\n1\n\u00132\n\u00b7\n\u0012F0\nF1\n\u0013\nand in general\n\u0012\nFn\nFn+1\n\u0013\n=\n\u0012\n0\n1\n1\n1\n\u0013n\n\u00b7\n\u0012\nF0\nF1\n\u0013\n.\nSo, in order to compute Fn, it suf\ufb01ces to raise this 2 \u00d7 2 matrix, call it X, to the nth power.\n(a) Show that two 2 \u00d7 2 matrices can be multiplied using 4 additions and 8 multiplications.\nBut how many matrix multiplications does it take to compute X n?\n(b) Show that O(log n) matrix multiplications suf\ufb01ce for computing X n. (Hint: Think about\ncomputing X8.)\nThus the number of arithmetic operations needed by our matrix-based algorithm, call it fib3, is\njust O(log n), as compared to O(n) for fib2. Have we broken another exponential barrier?\nThe catch is that our new algorithm involves multiplication, not just addition; and multiplica-\ntions of large numbers are slower than additions. We have already seen that, when the complex-\nity of arithmetic operations is taken into account, the running time of fib2 becomes O(n2).\n(c) Show that all intermediate results of fib3 are O(n) bits long.\n(d) Let M(n) be the running time of an algorithm for multiplying n-bit numbers, and assume\nthat M(n) = O(n2) (the school method for multiplication, recalled in Chapter 1, achieves\nthis). Prove that the running time of fib3 is O(M(n) log n).\n(e) Can you prove that the running time of fib3 is O(M(n))? (Hint: The lengths of the num-\nbers being multiplied get doubled with every squaring.)\nIn conclusion, whether fib3 is faster than fib2 depends on whether we can multiply n-bit\nintegers faster than O(n2). Do you think this is possible? (The answer is in Chapter 2.)\nFinally, there is a formula for the Fibonacci numbers:\nFn =\n1\n\u221a\n5\n \n1 +\n\u221a\n5\n2\n!n\n\u22121\n\u221a\n5\n \n1 \u2212\n\u221a\n5\n2\n!n\n.\nSo, it would appear that we only need to raise a couple of numbers to the nth power in order to\ncompute Fn. The problem is that these numbers are irrational, and computing them to suf\ufb01cient\naccuracy is nontrivial. In fact, our matrix method fib3 can be seen as a roundabout way of\nraising these irrational numbers to the nth power. If you know your linear algebra, you should\nsee why. (Hint: What are the eigenvalues of the matrix X?)\n"
}