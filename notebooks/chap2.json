{
    "content": "Chapter 2\nDivide-and-conquer algorithms\nThe divide-and-conquer strategy solves a problem by:\n1. Breaking it into subproblems that are themselves smaller instances of the same type of\nproblem\n2. Recursively solving these subproblems\n3. Appropriately combining their answers\nThe real work is done piecemeal, in three different places: in the partitioning of problems\ninto subproblems; at the very tail end of the recursion, when the subproblems are so small\nthat they are solved outright; and in the gluing together of partial answers. These are held\ntogether and coordinated by the algorithm\u2019s core recursive structure.\nAs an introductory example, we\u2019ll see how this technique yields a new algorithm for multi-\nplying numbers, one that is much more ef\ufb01cient than the method we all learned in elementary\nschool!\n2.1\nMultiplication\nThe mathematician Carl Friedrich Gauss (1777\u20131855) once noticed that although the product\nof two complex numbers\n(a + bi)(c + di)\n=\nac \u2212bd + (bc + ad)i\nseems to involve four real-number multiplications, it can in fact be done with just three: ac,\nbd, and (a + b)(c + d), since\nbc + ad\n=\n(a + b)(c + d) \u2212ac \u2212bd.\nIn our big-O way of thinking, reducing the number of multiplications from four to three seems\nwasted ingenuity. But this modest improvement becomes very signi\ufb01cant when applied recur-\nsively.\n55\n56\nAlgorithms\nLet\u2019s move away from complex numbers and see how this helps with regular multiplica-\ntion. Suppose x and y are two n-bit integers, and assume for convenience that n is a power of\n2 (the more general case is hardly any different). As a \ufb01rst step toward multiplying x and y,\nsplit each of them into their left and right halves, which are n/2 bits long:\nx =\nxL\nxR\n= 2n/2xL + xR\ny =\nyL\nyR\n= 2n/2yL + yR.\nFor instance, if x = 101101102 (the subscript 2 means \u201cbinary\u201d) then xL = 10112, xR = 01102,\nand x = 10112 \u00d7 24 + 01102. The product of x and y can then be rewritten as\nxy = (2n/2xL + xR)(2n/2yL + yR) = 2n xLyL + 2n/2 (xLyR + xRyL) + xRyR.\nWe will compute xy via the expression on the right. The additions take linear time, as do the\nmultiplications by powers of 2 (which are merely left-shifts). The signi\ufb01cant operations are\nthe four n/2-bit multiplications, xLyL, xLyR, xRyL, xRyR; these we can handle by four recursive\ncalls. Thus our method for multiplying n-bit numbers starts by making recursive calls to\nmultiply these four pairs of n/2-bit numbers (four subproblems of half the size), and then\nevaluates the preceding expression in O(n) time. Writing T(n) for the overall running time\non n-bit inputs, we get the recurrence relation\nT(n) = 4T(n/2) + O(n).\nWe will soon see general strategies for solving such equations. In the meantime, this particu-\nlar one works out to O(n2), the same running time as the traditional grade-school multiplica-\ntion technique. So we have a radically new algorithm, but we haven\u2019t yet made any progress\nin ef\ufb01ciency. How can our method be sped up?\nThis is where Gauss\u2019s trick comes to mind. Although the expression for xy seems to de-\nmand four n/2-bit multiplications, as before just three will do: xLyL, xRyR, and (xL+xR)(yL+yR),\nsince xLyR+xRyL = (xL+xR)(yL+yR)\u2212xLyL\u2212xRyR. The resulting algorithm, shown in Figure 2.1,\nhas an improved running time of1\nT(n) = 3T(n/2) + O(n).\nThe point is that now the constant factor improvement, from 4 to 3, occurs at every level of the\nrecursion, and this compounding effect leads to a dramatically lower time bound of O(n1.59).\nThis running time can be derived by looking at the algorithm\u2019s pattern of recursive calls,\nwhich form a tree structure, as in Figure 2.2. Let\u2019s try to understand the shape of this tree. At\neach successive level of recursion the subproblems get halved in size. At the (log2 n)th level,\n1Actually, the recurrence should read\nT(n) \u22643T(n/2 + 1) + O(n)\nsince the numbers (xL + xR) and (yL + yR) could be n/2 + 1 bits long. The one we\u2019re using is simpler to deal with\nand can be seen to imply exactly the same big-O running time.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n57\nFigure 2.1 A divide-and-conquer algorithm for integer multiplication.\nfunction multiply(x, y)\nInput:\nPositive integers x and y, in binary\nOutput:\nTheir product\nn = max(size of x, size of y)\nif n = 1:\nreturn xy\nxL, xR = leftmost \u2308n/2\u2309, rightmost \u230an/2\u230bbits of x\nyL, yR = leftmost \u2308n/2\u2309, rightmost \u230an/2\u230bbits of y\nP1 = multiply(xL, yL)\nP2 = multiply(xR, yR)\nP3 = multiply(xL + xR, yL + yR)\nreturn P1 \u00d7 2n + (P3 \u2212P1 \u2212P2) \u00d7 2n/2 + P2\nthe subproblems get down to size 1, and so the recursion ends. Therefore, the height of the\ntree is log2 n. The branching factor is 3\u2014each problem recursively produces three smaller\nones\u2014with the result that at depth k in the tree there are 3k subproblems, each of size n/2k.\nFor each subproblem, a linear amount of work is done in identifying further subproblems\nand combining their answers. Therefore the total time spent at depth k in the tree is\n3k \u00d7 O\n\u0010 n\n2k\n\u0011\n=\n\u00123\n2\n\u0013k\n\u00d7 O(n).\nAt the very top level, when k = 0, this works out to O(n). At the bottom, when k = log2 n,\nit is O(3log2 n), which can be rewritten as O(nlog2 3) (do you see why?). Between these two\nendpoints, the work done increases geometrically from O(n) to O(nlog2 3), by a factor of 3/2 per\nlevel. The sum of any increasing geometric series is, within a constant factor, simply the last\nterm of the series: such is the rapidity of the increase (Exercise 0.2). Therefore the overall\nrunning time is O(nlog2 3), which is about O(n1.59).\nIn the absence of Gauss\u2019s trick, the recursion tree would have the same height, but the\nbranching factor would be 4. There would be 4log2 n = n2 leaves, and therefore the running\ntime would be at least this much. In divide-and-conquer algorithms, the number of subprob-\nlems translates into the branching factor of the recursion tree; small changes in this coef\ufb01cient\ncan have a big impact on running time.\nA practical note: it generally does not make sense to recurse all the way down to 1 bit. For\nmost processors, 16- or 32-bit multiplication is a single operation, so by the time the numbers\nget into this range they should be handed over to the built-in procedure.\nFinally, the eternal question: Can we do better? It turns out that even faster algorithms\nfor multiplying numbers exist, based on another important divide-and-conquer algorithm: the\nfast Fourier transform, to be explained in Section 2.6.\n58\nAlgorithms\nFigure 2.2 Divide-and-conquer integer multiplication. (a) Each problem is divided into three\nsubproblems. (b) The levels of recursion.\n(a)\n10110010 \u00d7 01100011\n1011 \u00d7 0110\n0010 \u00d7 0011\n1101 \u00d7 1001\n(b)\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n1\n1\nSize n\nSize n/2\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n.\n.\n.\n.\n.\n.\nlog n\nlevels\nSize n/4\n2.2\nRecurrence relations\nDivide-and-conquer algorithms often follow a generic pattern: they tackle a problem of size\nn by recursively solving, say, a subproblems of size n/b and then combining these answers in\nO(nd) time, for some a, b, d > 0 (in the multiplication algorithm, a = 3, b = 2, and d = 1). Their\nrunning time can therefore be captured by the equation T(n) = aT(\u2308n/b\u2309) + O(nd). We next\nderive a closed-form solution to this general recurrence so that we no longer have to solve it\nexplicitly in each new instance.\nMaster theorem2 If T(n) = aT(\u2308n/b\u2309) + O(nd) for some constants a > 0, b > 1, and d \u22650,\n2There are even more general results of this type, but we will not be needing them.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n59\nFigure 2.3 Each problem of size n is divided into a subproblems of size n/b.\nSize 1\nSize n/b2\nSize n/b\nSize n\nDepth\nlogb n\nWidth alogb n = nlogb a\nBranching factor a\nthen\nT(n)\n=\n\uf8f1\n\uf8f2\n\uf8f3\nO(nd)\nif d > logb a\nO(nd log n)\nif d = logb a\nO(nlogb a)\nif d < logb a .\nThis single theorem tells us the running times of most of the divide-and-conquer procedures\nwe are likely to use.\nProof.\nTo prove the claim, let\u2019s start by assuming for the sake of convenience that n is a\npower of b. This will not in\ufb02uence the \ufb01nal bound in any important way\u2014after all, n is at\nmost a multiplicative factor of b away from some power of b (Exercise 2.2)\u2014and it will allow\nus to ignore the rounding effect in \u2308n/b\u2309.\nNext, notice that the size of the subproblems decreases by a factor of b with each level\nof recursion, and therefore reaches the base case after logb n levels. This is the height of\nthe recursion tree. Its branching factor is a, so the kth level of the tree is made up of ak\nsubproblems, each of size n/bk (Figure 2.3). The total work done at this level is\nak \u00d7 O\n\u0010 n\nbk\n\u0011d\n=\nO(nd) \u00d7\n\u0010 a\nbd\n\u0011k\n.\nAs k goes from 0 (the root) to logb n (the leaves), these numbers form a geometric series with\n60\nAlgorithms\nratio a/bd. Finding the sum of such a series in big-O notation is easy (Exercise 0.2), and comes\ndown to three cases.\n1. The ratio is less than 1.\nThen the series is decreasing, and its sum is just given by its \ufb01rst term, O(nd).\n2. The ratio is greater than 1.\nThe series is increasing and its sum is given by its last term, O(nlogb a):\nnd \u0010 a\nbd\n\u0011logb n\n= nd\n\u0012 alogb n\n(blogb n)d\n\u0013\n= alogb n = a(loga n)(logb a) = nlogb a.\n3. The ratio is exactly 1.\nIn this case all O(log n) terms of the series are equal to O(nd).\nThese cases translate directly into the three contingencies in the theorem statement.\nBinary search\nThe ultimate divide-and-conquer algorithm is, of course, binary search: to \ufb01nd a key k in a\nlarge \ufb01le containing keys z[0, 1, . . . , n \u22121] in sorted order, we \ufb01rst compare k with z[n/2], and\ndepending on the result we recurse either on the \ufb01rst half of the \ufb01le, z[0, . . . , n/2 \u22121], or on\nthe second half, z[n/2, . . . , n \u22121]. The recurrence now is T(n) = T(\u2308n/2\u2309) + O(1), which is the\ncase a = 1, b = 2, d = 0. Plugging into our master theorem we get the familiar solution: a\nrunning time of just O(log n).\n2.3\nMergesort\nThe problem of sorting a list of numbers lends itself immediately to a divide-and-conquer\nstrategy: split the list into two halves, recursively sort each half, and then merge the two\nsorted sublists.\nfunction mergesort(a[1 . . . n])\nInput:\nAn array of numbers a[1 . . . n]\nOutput:\nA sorted version of this array\nif n > 1:\nreturn merge(mergesort(a[1 . . .\u230an/2\u230b]), mergesort(a[\u230an/2\u230b+ 1 . . . n]))\nelse:\nreturn a\nThe correctness of this algorithm is self-evident, as long as a correct merge subroutine is\nspeci\ufb01ed. If we are given two sorted arrays x[1 . . . k] and y[1 . . . l], how do we ef\ufb01ciently merge\nthem into a single sorted array z[1 . . . k + l]? Well, the very \ufb01rst element of z is either x[1] or\ny[1], whichever is smaller. The rest of z[\u00b7] can then be constructed recursively.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n61\nFigure 2.4 The sequence of merge operations in mergesort.\n2\n3\n10\n1\n6\n7 13\n5\n10\n2\n5\n3\n13\n7\n1\n6\n2\n5\n3\n7\n13\n1\n6\n10\nInput:\n10\n2\n3\n1\n13\n5\n7\n6\n1\n6\n10 13\n3\n2\n5\n7\n.\nfunction merge(x[1 . . . k], y[1 . . . l])\nif k = 0:\nreturn y[1 . . . l]\nif l = 0:\nreturn x[1 . . . k]\nif x[1] \u2264y[1]:\nreturn x[1] \u25e6merge(x[2 . . . k], y[1 . . . l])\nelse:\nreturn y[1] \u25e6merge(x[1 . . . k], y[2 . . . l])\nHere \u25e6denotes concatenation. This merge procedure does a constant amount of work per\nrecursive call (provided the required array space is allocated in advance), for a total running\ntime of O(k + l). Thus merge\u2019s are linear, and the overall time taken by mergesort is\nT(n)\n=\n2T(n/2) + O(n),\nor O(n log n).\nLooking back at the mergesort algorithm, we see that all the real work is done in merg-\ning, which doesn\u2019t start until the recursion gets down to singleton arrays. The singletons are\nmerged in pairs, to yield arrays with two elements. Then pairs of these 2-tuples are merged,\nproducing 4-tuples, and so on. Figure 2.4 shows an example.\nThis viewpoint also suggests how mergesort might be made iterative. At any given mo-\nment, there is a set of \u201cactive\u201d arrays\u2014initially, the singletons\u2014which are merged in pairs to\ngive the next batch of active arrays. These arrays can be organized in a queue, and processed\nby repeatedly removing two arrays from the front of the queue, merging them, and putting\nthe result at the end of the queue.\n62\nAlgorithms\nIn the following pseudocode, the primitive operation inject adds an element to the end\nof the queue while eject removes and returns the element at the front of the queue.\nfunction iterative-mergesort(a[1 . . . n])\nInput:\nelements a1, a2, . . . , an to be sorted\nQ = [ ] (empty queue)\nfor i = 1 to n:\ninject(Q, [ai])\nwhile |Q| > 1:\ninject(Q, merge(eject(Q), eject(Q)))\nreturn eject(Q)\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n63\nAn n log n lower bound for sorting\nSorting algorithms can be depicted as trees. The one in the following \ufb01gure sorts an array of\nthree elements, a1, a2, a3. It starts by comparing a1 to a2 and, if the \ufb01rst is larger, compares\nit with a3; otherwise it compares a2 and a3. And so on. Eventually we end up at a leaf, and\nthis leaf is labeled with the true order of the three elements as a permutation of 1, 2, 3. For\nexample, if a2 < a1 < a3, we get the leaf labeled \u201c2 1 3.\u201d\n3 2 1\nYes\na2 < a3?\na1 < a2?\na1 < a3?\na2 < a3?\na1 < a3?\n2 3 1\n2 1 3\n3 1 2\n1 3 2\n1 2 3\nNo\nThe depth of the tree\u2014the number of comparisons on the longest path from root to leaf,\nin this case 3\u2014is exactly the worst-case time complexity of the algorithm.\nThis way of looking at sorting algorithms is useful because it allows one to argue that\nmergesort is optimal, in the sense that \u2126(n log n) comparisons are necessary for sorting n\nelements.\nHere is the argument: Consider any such tree that sorts an array of n elements. Each of\nits leaves is labeled by a permutation of {1, 2, . . . , n}. In fact, every permutation must appear\nas the label of a leaf. The reason is simple: if a particular permutation is missing, what\nhappens if we feed the algorithm an input ordered according to this same permutation? And\nsince there are n! permutations of n elements, it follows that the tree has at least n! leaves.\nWe are almost done: This is a binary tree, and we argued that it has at least n! leaves.\nRecall now that a binary tree of depth d has at most 2d leaves (proof: an easy induction on\nd). So, the depth of our tree\u2014and the complexity of our algorithm\u2014must be at least log(n!).\nAnd it is well known that log(n!) \u2265c \u00b7 n log n for some c > 0. There are many ways to see\nthis. The easiest is to notice that n! \u2265(n/2)(n/2) because n! = 1 \u00b7 2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 n contains at least\nn/2 factors larger than n/2; and to then take logs of both sides. Another is to recall Stirling\u2019s\nformula\nn! \u2248\ns\n\u03c0\n\u0012\n2n + 1\n3\n\u0013\n\u00b7 nn \u00b7 e\u2212n.\nEither way, we have established that any comparison tree that sorts n elements must make,\nin the worst case, \u2126(n log n) comparisons, and hence mergesort is optimal!\nWell, there is some \ufb01ne print: this neat argument applies only to algorithms that use\ncomparisons. Is it conceivable that there are alternative sorting strategies, perhaps using\nsophisticated numerical manipulations, that work in linear time? The answer is yes, under\ncertain exceptional circumstances: the canonical such example is when the elements to be\nsorted are integers that lie in a small range (Exercise 2.20).\n64\nAlgorithms\n2.4\nMedians\nThe median of a list of numbers is its 50th percentile: half the numbers are bigger than it,\nand half are smaller. For instance, the median of [45, 1, 10, 30, 25] is 25, since this is the middle\nelement when the numbers are arranged in order. If the list has even length, there are two\nchoices for what the middle element could be, in which case we pick the smaller of the two,\nsay.\nThe purpose of the median is to summarize a set of numbers by a single, typical value.\nThe mean, or average, is also very commonly used for this, but the median is in a sense more\ntypical of the data: it is always one of the data values, unlike the mean, and it is less sensitive\nto outliers. For instance, the median of a list of a hundred 1\u2019s is (rightly) 1, as is the mean.\nHowever, if just one of these numbers gets accidentally corrupted to 10,000, the mean shoots\nup above 100, while the median is unaffected.\nComputing the median of n numbers is easy: just sort them. The drawback is that this\ntakes O(n log n) time, whereas we would ideally like something linear. We have reason to be\nhopeful, because sorting is doing far more work than we really need\u2014we just want the middle\nelement and don\u2019t care about the relative ordering of the rest of them.\nWhen looking for a recursive solution, it is paradoxically often easier to work with a more\ngeneral version of the problem\u2014for the simple reason that this gives a more powerful step to\nrecurse upon. In our case, the generalization we will consider is selection.\nSELECTION\nInput: A list of numbers S; an integer k\nOutput: The kth smallest element of S\nFor instance, if k = 1, the minimum of S is sought, whereas if k = \u230a|S|/2\u230b, it is the median.\nA randomized divide-and-conquer algorithm for selection\nHere\u2019s a divide-and-conquer approach to selection. For any number v, imagine splitting list S\ninto three categories: elements smaller than v, those equal to v (there might be duplicates),\nand those greater than v. Call these SL, Sv, and SR respectively. For instance, if the array\nS :\n2\n36\n5\n21\n8\n13\n11\n20\n5\n4\n1\nis split on v = 5, the three subarrays generated are\nSL :\n2\n4\n1\nSv :\n5\n5\nSR :\n36\n21\n8\n13\n11\n20\nThe search can instantly be narrowed down to one of these sublists. If we want, say, the\neighth-smallest element of S, we know it must be the third-smallest element of SR since\n|SL| + |Sv| = 5. That is, selection(S, 8) = selection(SR, 3). More generally, by checking k\nagainst the sizes of the subarrays, we can quickly determine which of them holds the desired\nelement:\nselection(S, k) =\n\uf8f1\n\uf8f2\n\uf8f3\nselection(SL, k)\nif k \u2264|SL|\nv\nif |SL| < k \u2264|SL| + |Sv|\nselection(SR, k \u2212|SL| \u2212|Sv|)\nif k > |SL| + |Sv|.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n65\nThe three sublists SL, Sv, and SR can be computed from S in linear time; in fact, this compu-\ntation can even be done in place, that is, without allocating new memory (Exercise 2.15). We\nthen recurse on the appropriate sublist. The effect of the split is thus to shrink the number of\nelements from |S| to at most max{|SL|, |SR|}.\nOur divide-and-conquer algorithm for selection is now fully speci\ufb01ed, except for the crucial\ndetail of how to choose v. It should be picked quickly, and it should shrink the array substan-\ntially, the ideal situation being |SL|, |SR| \u22481\n2|S|. If we could always guarantee this situation,\nwe would get a running time of\nT(n) = T(n/2) + O(n),\nwhich is linear as desired. But this requires picking v to be the median, which is our ultimate\ngoal! Instead, we follow a much simpler alternative: we pick v randomly from S.\nEf\ufb01ciency analysis\nNaturally, the running time of our algorithm depends on the random choices of v. It is possible\nthat due to persistent bad luck we keep picking v to be the largest element of the array (or the\nsmallest element), and thereby shrink the array by only one element each time. In the earlier\nexample, we might \ufb01rst pick v = 36, then v = 21, and so on. This worst-case scenario would\nforce our selection algorithm to perform\nn + (n \u22121) + (n \u22122) + \u00b7 \u00b7 \u00b7 + n\n2 = \u0398(n2)\noperations (when computing the median), but it is extremely unlikely to occur. Equally un-\nlikely is the best possible case we discussed before, in which each randomly chosen v just\nhappens to split the array perfectly in half, resulting in a running time of O(n). Where, in\nthis spectrum from O(n) to \u0398(n2), does the average running time lie? Fortunately, it lies very\nclose to the best-case time.\nTo distinguish between lucky and unlucky choices of v, we will call v good if it lies within\nthe 25th to 75th percentile of the array that it is chosen from. We like these choices of v\nbecause they ensure that the sublists SL and SR have size at most three-fourths that of S (do\nyou see why?), so that the array shrinks substantially. Fortunately, good v\u2019s are abundant:\nhalf the elements of any list must fall between the 25th to 75th percentile!\nGiven that a randomly chosen v has a 50% chance of being good, how many v\u2019s do we need\nto pick on average before getting a good one? Here\u2019s a more familiar reformulation (see also\nExercise 1.34):\nLemma On average a fair coin needs to be tossed two times before a \u201cheads\u201d is seen.\nProof.\nLet E be the expected number of tosses before a heads is seen. We certainly need at\nleast one toss, and if it\u2019s heads, we\u2019re done. If it\u2019s tails (which occurs with probability 1/2), we\nneed to repeat. Hence E = 1 + 1\n2E, which works out to E = 2.\n66\nAlgorithms\nTherefore, after two split operations on average, the array will shrink to at most three-\nfourths of its size. Letting T(n) be the expected running time on an array of size n, we get\nT(n) \u2264T(3n/4) + O(n).\nThis follows by taking expected values of both sides of the following statement:\nTime taken on an array of size n\n\u2264\n(time taken on an array of size 3n/4) + (time to reduce array size to \u22643n/4),\nand, for the right-hand side, using the familiar property that the expectation of the sum is the\nsum of the expectations.\nFrom this recurrence we conclude that T(n) = O(n): on any input, our algorithm returns\nthe correct answer after a linear number of steps, on the average.\nThe Unix sort command\nComparing the algorithms for sorting and median-\ufb01nding we notice that, beyond the com-\nmon divide-and-conquer philosophy and structure, they are exact opposites. Mergesort splits\nthe array in two in the most convenient way (\ufb01rst half, second half), without any regard to\nthe magnitudes of the elements in each half; but then it works hard to put the sorted sub-\narrays together. In contrast, the median algorithm is careful about its splitting (smaller\nnumbers \ufb01rst, then the larger ones), but its work ends with the recursive call.\nQuicksort is a sorting algorithm that splits the array in exactly the same way as the me-\ndian algorithm; and once the subarrays are sorted, by two recursive calls, there is nothing\nmore to do. Its worst-case performance is \u0398(n2), like that of median-\ufb01nding. But it can be\nproved (Exercise 2.24) that its average case is O(n log n); furthermore, empirically it outper-\nforms other sorting algorithms. This has made quicksort a favorite in many applications\u2014\nfor instance, it is the basis of the code by which really enormous \ufb01les are sorted.\n2.5\nMatrix multiplication\nThe product of two n \u00d7 n matrices X and Y is a third n \u00d7 n matrix Z = XY , with (i, j)th entry\nZij =\nn\nX\nk=1\nXikYkj.\nTo make it more visual, Zij is the dot product of the ith row of X with the jth column of Y :\nX\nY\nZ\ni\nj\n(i, j)\n\u00d7\n=\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n67\nIn general, XY is not the same as Y X; matrix multiplication is not commutative.\nThe preceding formula implies an O(n3) algorithm for matrix multiplication: there are n2\nentries to be computed, and each takes O(n) time. For quite a while, this was widely believed\nto be the best running time possible, and it was even proved that in certain models of com-\nputation no algorithm could do better. It was therefore a source of great excitement when in\n1969, the German mathematician Volker Strassen announced a signi\ufb01cantly more ef\ufb01cient\nalgorithm, based upon divide-and-conquer.\nMatrix multiplication is particularly easy to break into subproblems, because it can be\nperformed blockwise. To see what this means, carve X into four n/2 \u00d7 n/2 blocks, and also Y :\nX =\n\u0014A\nB\nC\nD\n\u0015\n,\nY =\n\u0014E\nF\nG\nH\n\u0015\n.\nThen their product can be expressed in terms of these blocks and is exactly as if the blocks\nwere single elements (Exercise 2.11).\nXY\n=\n\u0014A\nB\nC\nD\n\u0015 \u0014E\nF\nG\nH\n\u0015\n=\n\u0014AE + BG\nAF + BH\nCE + DG\nCF + DH\n\u0015\nWe now have a divide-and-conquer strategy: to compute the size-n product XY , recursively\ncompute eight size-n/2 products AE, BG, AF, BH, CE, DG, CF, DH, and then do a few O(n2)-\ntime additions. The total running time is described by the recurrence relation\nT(n) = 8T(n/2) + O(n2).\nThis comes out to an unimpressive O(n3), the same as for the default algorithm. But the\nef\ufb01ciency can be further improved, and as with integer multiplication, the key is some clever\nalgebra. It turns out XY can be computed from just seven n/2 \u00d7 n/2 subproblems, via a\ndecomposition so tricky and intricate that one wonders how Strassen was ever able to discover\nit!\nXY\n=\n\u0014P5 + P4 \u2212P2 + P6\nP1 + P2\nP3 + P4\nP1 + P5 \u2212P3 \u2212P7\n\u0015\nwhere\nP1\n=\nA(F \u2212H)\nP2\n=\n(A + B)H\nP3\n=\n(C + D)E\nP4\n=\nD(G \u2212E)\nP5\n=\n(A + D)(E + H)\nP6\n=\n(B \u2212D)(G + H)\nP7\n=\n(A \u2212C)(E + F)\nThe new running time is\nT(n)\n=\n7T(n/2) + O(n2),\nwhich by the master theorem works out to O(nlog2 7) \u2248O(n2.81).\n68\nAlgorithms\n2.6\nThe fast Fourier transform\nWe have so far seen how divide-and-conquer gives fast algorithms for multiplying integers\nand matrices; our next target is polynomials. The product of two degree-d polynomials is a\npolynomial of degree 2d, for example:\n(1 + 2x + 3x2) \u00b7 (2 + x + 4x2) = 2 + 5x + 12x2 + 11x3 + 12x4.\nMore generally, if A(x) = a0 + a1x + \u00b7 \u00b7 \u00b7 + adxd and B(x) = b0 + b1x + \u00b7 \u00b7 \u00b7 + bdxd, their product\nC(x) = A(x) \u00b7 B(x) = c0 + c1x + \u00b7 \u00b7 \u00b7 + c2dx2d has coef\ufb01cients\nck = a0bk + a1bk\u22121 + \u00b7 \u00b7 \u00b7 + akb0 =\nk\nX\ni=0\naibk\u2212i\n(for i > d, take ai and bi to be zero). Computing ck from this formula takes O(k) steps, and\n\ufb01nding all 2d + 1 coef\ufb01cients would therefore seem to require \u0398(d2) time. Can we possibly\nmultiply polynomials faster than this?\nThe solution we will develop, the fast Fourier transform, has revolutionized\u2014indeed,\nde\ufb01ned\u2014the \ufb01eld of signal processing (see the following box). Because of its huge impor-\ntance, and its wealth of insights from different \ufb01elds of study, we will approach it a little\nmore leisurely than usual. The reader who wants just the core algorithm can skip directly to\nSection 2.6.4.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n69\nWhy multiply polynomials?\nFor one thing, it turns out that the fastest algorithms we have for multiplying integers rely\nheavily on polynomial multiplication; after all, polynomials and binary integers are quite\nsimilar\u2014just replace the variable x by the base 2, and watch out for carries. But perhaps\nmore importantly, multiplying polynomials is crucial for signal processing.\nA signal is any quantity that is a function of time (as in Figure (a)) or of position. It\nmight, for instance, capture a human voice by measuring \ufb02uctuations in air pressure close\nto the speaker\u2019s mouth, or alternatively, the pattern of stars in the night sky, by measuring\nbrightness as a function of angle.\na(t)\nt\n\u0000\u0001\u0000\u0002\n\u0003\u0001\u0003\u0004\n\u0005\u0001\u0005\n\u0006\u0001\u0006\n\u0007\u0001\u0007\b\n\t\n\u000b\f\n\r\u000e\n\u000f\u0001\u000f\n\u000f\u0001\u000f\u0010\n\u0010\n\u0011\u0001\u0011\n\u0012\u0001\u0012\n\u0013\u0001\u0013\u0014\n\u0015\u0016\n\u0017\u0001\u0017\n\u0018\u0001\u0018\n\u0019\u001a\n\u001b\u0001\u001b\u001c\n\u001d\u0001\u001d\n\u001e\u0001\u001e\n\u001f\u0001\u001f\n \u0001 \n!\u0001!\"\n#\u0001#\n$\u0001$\n%\u0001%&\n'\u0001'\n(\u0001(\n)*\n+\u0001+,\n-\u0001-.\n/\u0001/\n0\u00010\n1\u00011\n2\u00012\n3\u000134\n5\u00015\n6\u00016\n78\n9\u00019\n:\u0001:\n;\u0001;\n<\u0001<\n=\u0001=>\n?\u0001?@\na(t)\nt\nAB\nCD\nEF\nGH\nIJ\nKL\nMN\nOP\nQR\nSTS\nUTU\nVW\nXY\nZ[\n\\]\n^_\n`a\nbc\nde\nfTfg\nhi\njk\nlTlm\nno\npq\nt\n\u03b4(t)\n(a)\n(b)\n(c)\nIn order to extract information from a signal, we need to \ufb01rst digitize it by sampling\n(Figure (b))\u2014and, then, to put it through a system that will transform it in some way. The\noutput is called the response of the system:\nsignal\n\u2212\n\u2192\nSYSTEM\n\u2212\n\u2192\nresponse\nAn important class of systems are those that are linear\u2014the response to the sum of two\nsignals is just the sum of their individual responses\u2014and time invariant\u2014shifting the input\nsignal by time t produces the same output, also shifted by t. Any system with these prop-\nerties is completely characterized by its response to the simplest possible input signal: the\nunit impulse \u03b4(t), consisting solely of a \u201cjerk\u201d at t = 0 (Figure (c)). To see this, \ufb01rst consider\nthe close relative \u03b4(t \u2212i), a shifted impulse in which the jerk occurs at time i. Any signal\na(t) can be expressed as a linear combination of these, letting \u03b4(t \u2212i) pick out its behavior\nat time i,\na(t) =\nT\u22121\nX\ni=0\na(i)\u03b4(t \u2212i)\n(if the signal consists of T samples). By linearity, the system response to input a(t) is deter-\nmined by the responses to the various \u03b4(t\u2212i). And by time invariance, these are in turn just\nshifted copies of the impulse response b(t), the response to \u03b4(t).\nIn other words, the output of the system at time k is\nc(k) =\nk\nX\ni=0\na(i)b(k \u2212i),\nexactly the formula for polynomial multiplication!\n70\nAlgorithms\n2.6.1\nAn alternative representation of polynomials\nTo arrive at a fast algorithm for polynomial multiplication we take inspiration from an impor-\ntant property of polynomials.\nFact\nA degree-d polynomial is uniquely characterized by its values at any d + 1 distinct\npoints.\nA familiar instance of this is that \u201cany two points determine a line.\u201d We will later see why\nthe more general statement is true (page 76), but for the time being it gives us an alternative\nrepresentation of polynomials. Fix any distinct points x0, . . . , xd. We can specify a degree-d\npolynomial A(x) = a0 + a1x + \u00b7 \u00b7 \u00b7 + adxd by either one of the following:\n1. Its coef\ufb01cients a0, a1, . . . , ad\n2. The values A(x0), A(x1), . . . , A(xd)\nOf these two representations, the second is the more attractive for polynomial multiplication.\nSince the product C(x) has degree 2d, it is completely determined by its value at any 2d + 1\npoints. And its value at any given point z is easy enough to \ufb01gure out, just A(z) times B(z).\nThus polynomial multiplication takes linear time in the value representation.\nThe problem is that we expect the input polynomials, and also their product, to be speci\ufb01ed\nby coef\ufb01cients. So we need to \ufb01rst translate from coef\ufb01cients to values\u2014which is just a matter\nof evaluating the polynomial at the chosen points\u2014then multiply in the value representation,\nand \ufb01nally translate back to coef\ufb01cients, a process called interpolation.\nInterpolation\nCoef\ufb01cient representation\na0, a1, . . . , ad\nValue representation\nA(x0), A(x1), . . . , A(xd)\nEvaluation\nFigure 2.5 presents the resulting algorithm.\nThe equivalence of the two polynomial representations makes it clear that this high-level\napproach is correct, but how ef\ufb01cient is it? Certainly the selection step and the n multiplica-\ntions are no trouble at all, just linear time.3 But (leaving aside interpolation, about which we\nknow even less) how about evaluation? Evaluating a polynomial of degree d \u2264n at a single\npoint takes O(n) steps (Exercise 2.29), and so the baseline for n points is \u0398(n2). We\u2019ll now see\nthat the fast Fourier transform (FFT) does it in just O(n log n) time, for a particularly clever\nchoice of x0, . . . , xn\u22121 in which the computations required by the individual points overlap with\none another and can be shared.\n3In a typical setting for polynomial multiplication, the coef\ufb01cients of the polynomials are real numbers and,\nmoreover, are small enough that basic arithmetic operations (adding and multiplying) take unit time. We will\nassume this to be the case without any great loss of generality; in particular, the time bounds we obtain are easily\nadjustable to situations with larger numbers.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n71\nFigure 2.5 Polynomial multiplication\nInput:\nCoefficients of two polynomials, A(x) and B(x), of degree d\nOutput:\nTheir product C = A \u00b7 B\nSelection\nPick some points x0, x1, . . . , xn\u22121, where n \u22652d + 1\nEvaluation\nCompute A(x0), A(x1), . . . , A(xn\u22121) and B(x0), B(x1), . . . , B(xn\u22121)\nMultiplication\nCompute C(xk) = A(xk)B(xk) for all k = 0, . . . , n \u22121\nInterpolation\nRecover C(x) = c0 + c1x + \u00b7 \u00b7 \u00b7 + c2dx2d\n2.6.2\nEvaluation by divide-and-conquer\nHere\u2019s an idea for how to pick the n points at which to evaluate a polynomial A(x) of degree\n\u2264n \u22121. If we choose them to be positive-negative pairs, that is,\n\u00b1x0, \u00b1x1, . . . , \u00b1xn/2\u22121,\nthen the computations required for each A(xi) and A(\u2212xi) overlap a lot, because the even\npowers of xi coincide with those of \u2212xi.\nTo investigate this, we need to split A(x) into its odd and even powers, for instance\n3 + 4x + 6x2 + 2x3 + x4 + 10x5 = (3 + 6x2 + x4) + x(4 + 2x2 + 10x4).\nNotice that the terms in parentheses are polynomials in x2. More generally,\nA(x) = Ae(x2) + xAo(x2),\nwhere Ae(\u00b7), with the even-numbered coef\ufb01cients, and Ao(\u00b7), with the odd-numbered coef\ufb01-\ncients, are polynomials of degree \u2264n/2 \u22121 (assume for convenience that n is even). Given\npaired points \u00b1xi, the calculations needed for A(xi) can be recycled toward computing A(\u2212xi):\nA(xi)\n=\nAe(x2\ni ) + xiAo(x2\ni )\nA(\u2212xi)\n=\nAe(x2\ni ) \u2212xiAo(x2\ni ).\nIn other words, evaluating A(x) at n paired points \u00b1x0, . . . , \u00b1xn/2\u22121 reduces to evaluating\nAe(x) and Ao(x) (which each have half the degree of A(x)) at just n/2 points, x2\n0, . . . , x2\nn/2\u22121.\n72\nAlgorithms\nEvaluate:\nA(x)\ndegree \u2264n \u22121\nAe(x) and Ao(x)\ndegree \u2264n/2 \u22121\nat:\nat:\n\u2212x0\n+x1\n\u2212x1\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nx2\n0\n\u2212xn/2\u22121\n+xn/2\u22121\nx2\n1\nx2\nn/2\u22121\n+x0\nEquivalently,\nevaluate:\nThe original problem of size n is in this way recast as two subproblems of size n/2, followed\nby some linear-time arithmetic. If we could recurse, we would get a divide-and-conquer pro-\ncedure with running time\nT(n) = 2T(n/2) + O(n),\nwhich is O(n log n), exactly what we want.\nBut we have a problem: The plus-minus trick only works at the top level of the recur-\nsion. To recurse at the next level, we need the n/2 evaluation points x2\n0, x2\n1, . . . , x2\nn/2\u22121 to be\nthemselves plus-minus pairs. But how can a square be negative? The task seems impossible!\nUnless, of course, we use complex numbers.\nFine, but which complex numbers? To \ufb01gure this out, let us \u201creverse engineer\u201d the process.\nAt the very bottom of the recursion, we have a single point. This point might as well be 1, in\nwhich case the level above it must consist of its square roots, \u00b1\n\u221a\n1 = \u00b11.\n\u22121\n\u2212i\n\u22121\n+1\n+1\n+i\n+1\n.\n.\n.\nThe next level up then has \u00b1\u221a+1 = \u00b11 as well as the complex numbers \u00b1\u221a\u22121 = \u00b1i, where i\nis the imaginary unit. By continuing in this manner, we eventually reach the initial set of n\npoints. Perhaps you have already guessed what they are: the complex nth roots of unity, that\nis, the n complex solutions to the equation zn = 1.\nFigure 2.6 is a pictorial review of some basic facts about complex numbers. The third panel\nof this \ufb01gure introduces the nth roots of unity: the complex numbers 1, \u03c9, \u03c92, . . . , \u03c9n\u22121, where\n\u03c9 = e2\u03c0i/n. If n is even,\n1. The nth roots are plus-minus paired, \u03c9n/2+j = \u2212\u03c9j.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n73\n2. Squaring them produces the (n/2)nd roots of unity.\nTherefore, if we start with these numbers for some n that is a power of 2, then at successive\nlevels of recursion we will have the (n/2k)th roots of unity, for k = 0, 1, 2, 3, . . .. All these sets\nof numbers are plus-minus paired, and so our divide-and-conquer, as shown in the last panel,\nworks perfectly. The resulting algorithm is the fast Fourier transform (Figure 2.7).\n74\nAlgorithms\nFigure 2.6 The complex roots of unity are ideal for our divide-and-conquer scheme.\n\u03b8\nReal\nImaginary\na\nb\nr\nThe complex plane\nz = a + bi is plotted at position (a, b).\nPolar coordinates: rewrite as z = r(cos \u03b8 + i sin \u03b8) = rei\u03b8,\ndenoted (r, \u03b8).\n\u2022 length r =\n\u221a\na2 + b2.\n\u2022 angle \u03b8 \u2208[0, 2\u03c0): cos \u03b8 = a/r, sin \u03b8 = b/r.\n\u2022 \u03b8 can always be reduced modulo 2\u03c0.\nExamples:\nNumber\n\u22121\ni\n5 + 5i\nPolar coords\n(1, \u03c0)\n(1, \u03c0/2)\n(5\n\u221a\n2, \u03c0/4)\n(r1r2, \u03b81 + \u03b82)\n(r1, \u03b81)\n(r2, \u03b82)\nMultiplying is easy in polar coordinates\nMultiply the lengths and add the angles:\n(r1, \u03b81) \u00d7 (r2, \u03b82) = (r1r2, \u03b81 + \u03b82).\nFor any z = (r, \u03b8),\n\u2022 \u2212z = (r, \u03b8 + \u03c0) since \u22121 = (1, \u03c0).\n\u2022 If z is on the unit circle (i.e., r = 1), then zn = (1, n\u03b8).\nAngle 2\u03c0\nn\n4\u03c0\nn\n2\u03c0\nn + \u03c0\nThe nth complex roots of unity\nSolutions to the equation zn = 1.\nBy the multiplication rule: solutions are z = (1, \u03b8), for \u03b8 a\nmultiple of 2\u03c0/n (shown here for n = 16).\nFor even n:\n\u2022 These numbers are plus-minus paired: \u2212(1, \u03b8) = (1, \u03b8+\u03c0).\n\u2022 Their squares are the (n/2)nd roots of unity, shown here\nwith boxes around them.\nDivide-and-conquer step\nEvaluate\nAe(x), Ao(x)\nat (n/2)nd\nroots\nStill\npaired\nDivide and\nconquer\nPaired\nEvaluate A(x)\nat nth roots\nof unity\n(n is a power of 2)\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n75\nFigure 2.7 The fast Fourier transform (polynomial formulation)\nfunction FFT(A, \u03c9)\nInput:\nCoefficient representation of a polynomial A(x)\nof degree \u2264n \u22121, where n is a power of 2\n\u03c9, an nth root of unity\nOutput:\nValue representation A(\u03c90), . . . , A(\u03c9n\u22121)\nif \u03c9 = 1:\nreturn A(1)\nexpress A(x) in the form Ae(x2) + xAo(x2)\ncall FFT(Ae, \u03c92) to evaluate Ae at even powers of \u03c9\ncall FFT(Ao, \u03c92) to evaluate Ao at even powers of \u03c9\nfor j = 0 to n \u22121:\ncompute A(\u03c9j) = Ae(\u03c92j) + \u03c9jAo(\u03c92j)\nreturn A(\u03c90), . . . , A(\u03c9n\u22121)\n2.6.3\nInterpolation\nLet\u2019s take stock of where we are.\nWe \ufb01rst developed a high-level scheme for multiplying\npolynomials (Figure 2.5), based on the observation that polynomials can be represented in\ntwo ways, in terms of their coef\ufb01cients or in terms of their values at a selected set of points.\nInterpolation\nCoef\ufb01cient representation\na0, a1, . . . , an\u22121\nValue representation\nA(x0), A(x1), . . . , A(xn\u22121)\nEvaluation\nThe value representation makes it trivial to multiply polynomials, but we cannot ignore the\ncoef\ufb01cient representation since it is the form in which the input and output of our overall\nalgorithm are speci\ufb01ed.\nSo we designed the FFT, a way to move from coef\ufb01cients to values in time just O(n log n),\nwhen the points {xi} are complex nth roots of unity (1, \u03c9, \u03c92, . . . , \u03c9n\u22121).\n\u27e8values\u27e9= FFT(\u27e8coef\ufb01cients\u27e9, \u03c9).\nThe last remaining piece of the puzzle is the inverse operation, interpolation. It will turn out,\namazingly, that\n\u27e8coef\ufb01cients\u27e9= 1\nn FFT(\u27e8values\u27e9, \u03c9\u22121).\nInterpolation is thus solved in the most simple and elegant way we could possibly have hoped\nfor\u2014using the same FFT algorithm, but called with \u03c9\u22121 in place of \u03c9! This might seem like a\nmiraculous coincidence, but it will make a lot more sense when we recast our polynomial oper-\nations in the language of linear algebra. Meanwhile, our O(n log n) polynomial multiplication\nalgorithm (Figure 2.5) is now fully speci\ufb01ed.\n76\nAlgorithms\nA matrix reformulation\nTo get a clearer view of interpolation, let\u2019s explicitly set down the relationship between our two\nrepresentations for a polynomial A(x) of degree \u2264n \u22121. They are both vectors of n numbers,\nand one is a linear transformation of the other:\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\nA(x0)\nA(x1)\n.\n.\n.\nA(xn\u22121)\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n=\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n1\nx0\nx2\n0\n\u00b7 \u00b7 \u00b7\nxn\u22121\n0\n1\nx1\nx2\n1\n\u00b7 \u00b7 \u00b7\nxn\u22121\n1\n.\n.\n.\n1\nxn\u22121\nx2\nn\u22121\n\u00b7 \u00b7 \u00b7\nxn\u22121\nn\u22121\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\na0\na1\n.\n.\n.\nan\u22121\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb.\nCall the matrix in the middle M. Its specialized format\u2014a Vandermonde matrix\u2014gives it\nmany remarkable properties, of which the following is particularly relevant to us.\nIf x0, . . . , xn\u22121 are distinct numbers, then M is invertible.\nThe existence of M \u22121 allows us to invert the preceding matrix equation so as to express coef-\n\ufb01cients in terms of values. In brief,\nEvaluation is multiplication by M, while interpolation is multiplication by M \u22121.\nThis reformulation of our polynomial operations reveals their essential nature more clearly.\nAmong other things, it \ufb01nally justi\ufb01es an assumption we have been making throughout, that\nA(x) is uniquely characterized by its values at any n points\u2014in fact, we now have an explicit\nformula that will give us the coef\ufb01cients of A(x) in this situation. Vandermonde matrices also\nhave the distinction of being quicker to invert than more general matrices, in O(n2) time in-\nstead of O(n3). However, using this for interpolation would still not be fast enough for us, so\nonce again we turn to our special choice of points\u2014the complex roots of unity.\nInterpolation resolved\nIn linear algebra terms, the FFT multiplies an arbitrary n-dimensional vector\u2014which we\nhave been calling the coef\ufb01cient representation\u2014by the n \u00d7 n matrix\nMn(\u03c9) =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n1\n1\n1\n\u00b7 \u00b7 \u00b7\n1\n1\n\u03c9\n\u03c92\n\u00b7 \u00b7 \u00b7\n\u03c9n\u22121\n1\n\u03c92\n\u03c94\n\u00b7 \u00b7 \u00b7\n\u03c92(n\u22121)\n.\n.\n.\n1\n\u03c9j\n\u03c92j\n\u00b7 \u00b7 \u00b7\n\u03c9(n\u22121)j\n.\n.\n.\n1\n\u03c9(n\u22121)\n\u03c92(n\u22121)\n\u00b7 \u00b7 \u00b7\n\u03c9(n\u22121)(n\u22121)\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\u2190\n\u2212\nrow for \u03c90 = 1\n\u2190\n\u2212\n\u03c9\n\u2190\n\u2212\n\u03c92\n.\n.\n.\n\u2190\n\u2212\n\u03c9j\n.\n.\n.\n\u2190\n\u2212\n\u03c9n\u22121\nwhere \u03c9 is a complex nth root of unity, and n is a power of 2. Notice how simple this matrix is\nto describe: its (j, k)th entry (starting row- and column-count at zero) is \u03c9jk.\nMultiplication by M = Mn(\u03c9) maps the kth coordinate axis (the vector with all zeros except\nfor a 1 at position k) onto the kth column of M. Now here\u2019s the crucial observation, which we\u2019ll\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n77\nFigure 2.8 The FFT takes points in the standard coordinate system, whose axes are shown\nhere as x1, x2, x3, and rotates them into the Fourier basis, whose axes are the columns of\nMn(\u03c9), shown here as f1, f2, f3. For instance, points in direction x1 get mapped into direction\nf1.\nFFT\nx1\nx3\nx2\nf3\nf1\nf2\nprove shortly: the columns of M are orthogonal (at right angles) to each other. Therefore\nthey can be thought of as the axes of an alternative coordinate system, which is often called\nthe Fourier basis. The effect of multiplying a vector by M is to rotate it from the standard\nbasis, with the usual set of axes, into the Fourier basis, which is de\ufb01ned by the columns of\nM (Figure 2.8). The FFT is thus a change of basis, a rigid rotation. The inverse of M is the\nopposite rotation, from the Fourier basis back into the standard basis. When we write out the\northogonality condition precisely, we will be able to read off this inverse transformation with\nease:\nInversion formula\nMn(\u03c9)\u22121 =\n1\nnMn(\u03c9\u22121).\nBut \u03c9\u22121 is also an nth root of unity, and so interpolation\u2014or equivalently, multiplication by\nMn(\u03c9)\u22121\u2014is itself just an FFT operation, but with \u03c9 replaced by \u03c9\u22121.\nNow let\u2019s get into the details. Take \u03c9 to be e2\u03c0i/n for convenience, and think of the columns\nof M as vectors in Cn. Recall that the angle between two vectors u = (u0, . . . , un\u22121) and\nv = (v0, . . . , vn\u22121) in Cn is just a scaling factor times their inner product\nu \u00b7 v\u2217= u0v\u2217\n0 + u1v\u2217\n1 + \u00b7 \u00b7 \u00b7 + un\u22121v\u2217\nn\u22121,\nwhere z\u2217denotes the complex conjugate4 of z. This quantity is maximized when the vectors\nlie in the same direction and is zero when the vectors are orthogonal to each other.\nThe fundamental observation we need is the following.\nLemma The columns of matrix M are orthogonal to each other.\nProof. Take the inner product of any columns j and k of matrix M,\n1 + \u03c9j\u2212k + \u03c92(j\u2212k) + \u00b7 \u00b7 \u00b7 + \u03c9(n\u22121)(j\u2212k).\n4The complex conjugate of a complex number z = rei\u03b8 is z\u2217= re\u2212i\u03b8. The complex conjugate of a vector (or\nmatrix) is obtained by taking the complex conjugates of all its entries.\n78\nAlgorithms\nThis is a geometric series with \ufb01rst term 1, last term \u03c9(n\u22121)(j\u2212k), and ratio \u03c9(j\u2212k). Therefore it\nevaluates to (1 \u2212\u03c9n(j\u2212k))/(1 \u2212\u03c9(j\u2212k)), which is 0\u2014except when j = k, in which case all terms\nare 1 and the sum is n.\nThe orthogonality property can be summarized in the single equation\nMM\u2217= nI,\nsince (MM \u2217)ij is the inner product of the ith and jth columns of M (do you see why?). This\nimmediately implies M \u22121 = (1/n)M \u2217: we have an inversion formula! But is it the same for-\nmula we earlier claimed? Let\u2019s see\u2014the (j, k)th entry of M \u2217is the complex conjugate of the\ncorresponding entry of M, in other words \u03c9\u2212jk. Whereupon M \u2217= Mn(\u03c9\u22121), and we\u2019re done.\nAnd now we can \ufb01nally step back and view the whole affair geometrically. The task we\nneed to perform, polynomial multiplication, is a lot easier in the Fourier basis than in the\nstandard basis. Therefore, we \ufb01rst rotate vectors into the Fourier basis (evaluation), then\nperform the task (multiplication), and \ufb01nally rotate back (interpolation). The initial vectors\nare coef\ufb01cient representations, while their rotated counterparts are value representations. To\nef\ufb01ciently switch between these, back and forth, is the province of the FFT.\n2.6.4\nA closer look at the fast Fourier transform\nNow that our ef\ufb01cient scheme for polynomial multiplication is fully realized, let\u2019s hone in\nmore closely on the core subroutine that makes it all possible, the fast Fourier transform.\nThe de\ufb01nitive FFT algorithm\nThe FFT takes as input a vector a = (a0, . . . , an\u22121) and a complex number \u03c9 whose powers\n1, \u03c9, \u03c92, . . . , \u03c9n\u22121 are the complex nth roots of unity. It multiplies vector a by the n \u00d7 n matrix\nMn(\u03c9), which has (j, k)th entry (starting row- and column-count at zero) \u03c9jk. The potential\nfor using divide-and-conquer in this matrix-vector multiplication becomes apparent when M\u2019s\ncolumns are segregated into evens and odds:\n=\na\nMn(\u03c9)\nan\u22121\na0\na1\na2\na3\na4\n.\n.\n.\n\u03c9jk\nk\nj\n=\na2\na1\na3\nan\u22121\n.\n.\n.\na0\n.\n.\n.\nan\u22122\n2k + 1\nColumn\n2k\nEven\n\u03c92jk\n\u03c9j \u00b7 \u03c92jk\ncolumns\nOdd\ncolumns\nj\nRow j\na2\na1\na3\nan\u22121\n.\n.\n.\na0\n.\n.\n.\nan\u22122\n\u03c92jk\n\u03c92jk\n\u03c9j \u00b7 \u03c92jk\n2k + 1\nColumn\nj + n/2\n2k\n\u2212\u03c9j \u00b7 \u03c92jk\nIn the second step, we have simpli\ufb01ed entries in the bottom half of the matrix using \u03c9n/2 = \u22121\nand \u03c9n = 1. Notice that the top left n/2 \u00d7 n/2 submatrix is Mn/2(\u03c92), as is the one on the\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n79\nFigure 2.9 The fast Fourier transform\nfunction FFT(a, \u03c9)\nInput:\nAn array a = (a0, a1, . . . , an\u22121), for n a power of 2\nA primitive nth root of unity, \u03c9\nOutput:\nMn(\u03c9) a\nif \u03c9 = 1:\nreturn a\n(s0, s1, . . . , sn/2\u22121) = FFT((a0, a2, . . . , an\u22122), \u03c92)\n(s\u2032\n0, s\u2032\n1, . . . , s\u2032\nn/2\u22121) = FFT((a1, a3, . . . , an\u22121), \u03c92)\nfor j = 0 to n/2 \u22121:\nrj = sj + \u03c9js\u2032\nj\nrj+n/2 = sj \u2212\u03c9js\u2032\nj\nreturn (r0, r1, . . . , rn\u22121)\nbottom left. And the top and bottom right submatrices are almost the same as Mn/2(\u03c92),\nbut with their jth rows multiplied through by \u03c9j and \u2212\u03c9j, respectively. Therefore the \ufb01nal\nproduct is the vector\na0\na2\n.\n.\n.\nan\u22122\na0\na2\n.\n.\n.\nan\u22122\nMn/2\nMn/2\na1\na3\n.\n.\n.\nan\u22121\na1\na3\n.\n.\n.\nan\u22121\nMn/2\nMn/2\n+ \u03c9j\n\u2212\u03c9j\nj + n/2\nRow j\nIn short, the product of Mn(\u03c9) with vector (a0, . . . , an\u22121), a size-n problem, can be expressed\nin terms of two size-n/2 problems: the product of Mn/2(\u03c92) with (a0, a2, . . . , an\u22122) and with\n(a1, a3, . . . , an\u22121). This divide-and-conquer strategy leads to the de\ufb01nitive FFT algorithm of\nFigure 2.9, whose running time is T(n) = 2T(n/2) + O(n) = O(n log n).\nThe fast Fourier transform unraveled\nThroughout all our discussions so far, the fast Fourier transform has remained tightly co-\ncooned within a divide-and-conquer formalism. To fully expose its structure, we now unravel\nthe recursion.\nThe divide-and-conquer step of the FFT can be drawn as a very simple circuit. Here is how\na problem of size n is reduced to two subproblems of size n/2 (for clarity, one pair of outputs\n(j, j + n/2) is singled out):\n80\nAlgorithms\na0\na2\na3\nj + n/2\nj\na1\nan\u22121\nrj+n/2\nFFTn/2\nFFTn/2\n.\n.\n.\n.\n.\n.\nan\u22122\nrj\nFFTn (input: a0, . . . , an\u22121, output: r0, . . . , rn\u22121)\nWe\u2019re using a particular shorthand: the edges are wires carrying complex numbers from left\nto right. A weight of j means \u201cmultiply the number on this wire by \u03c9j.\u201d And when two wires\ncome into a junction from the left, the numbers they are carrying get added up. So the two\noutputs depicted are executing the commands\nrj\n=\nsj + \u03c9js\u2032\nj\nrj+n/2\n=\nsj \u2212\u03c9js\u2032\nj\nfrom the FFT algorithm (Figure 2.9), via a pattern of wires known as a butter\ufb02y:\n.\nUnraveling the FFT circuit completely for n = 8 elements, we get Figure 10.4. Notice the\nfollowing.\n1. For n inputs there are log2 n levels, each with n nodes, for a total of n log n operations.\n2. The inputs are arranged in a peculiar order: 0, 4, 2, 6, 1, 5, 3, 7.\nWhy? Recall that at the top level of recursion, we \ufb01rst bring up the even coef\ufb01cients of the\ninput and then move on to the odd ones. Then at the next level, the even coef\ufb01cients of this\n\ufb01rst group (which therefore are multiples of 4, or equivalently, have zero as their two least\nsigni\ufb01cant bits) are brought up, and so on. To put it otherwise, the inputs are arranged by\nincreasing last bit of the binary representation of their index, resolving ties by looking at the\nnext more signi\ufb01cant bit(s). The resulting order in binary, 000, 100, 010, 110, 001, 101, 011, 111,\nis the same as the natural one, 000, 001, 010, 011, 100, 101, 110, 111 except the bits are mirrored!\n3. There is a unique path between each input aj and each output A(\u03c9k).\nThis path is most easily described using the binary representations of j and k (shown in\nFigure 10.4 for convenience). There are two edges out of each node, one going up (the 0-edge)\nand one going down (the 1-edge). To get to A(\u03c9k) from any input node, simply follow the edges\nspeci\ufb01ed in the bit representation of k, starting from the rightmost bit. (Can you similarly\nspecify the path in the reverse direction?)\n4. On the path between aj and A(\u03c9k), the labels add up to jk mod 8.\nSince \u03c98 = 1, this means that the contribution of input aj to output A(\u03c9k) is aj\u03c9jk, and\ntherefore the circuit computes correctly the values of polynomial A(x).\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n81\n5. And \ufb01nally, notice that the FFT circuit is a natural for parallel computation and direct\nimplementation in hardware.\nFigure 2.10 The fast Fourier transform circuit.\n\u0000\u0001\n\u0002\u0003\n\u0004\u0005\n\u0006\u0007\n\b\t\n\n\u000b\n\f\r\n\u000e\u000f\n\u0010\u0011\n\u0012\u0013\n\u0014\u0015\n\u0016\u0017\n\u0018\u0019\n\u001a\u001b\n\u001c\u001d\n\u001e\u001f\n !\n\"#\n$%\n&'\n()\n*+\n,-\n./\na0\na4\na2\na6\na1\na5\na7\nA(\u03c91)\nA(\u03c92)\nA(\u03c93)\nA(\u03c94)\nA(\u03c95)\nA(\u03c96)\nA(\u03c97)\na3\nA(\u03c90)\n1\n4\n4\n4\n4\n6\n6\n7\n4\n4\n2\n2\n6\n3\n2\n5\n4\n000\n100\n010\n110\n001\n101\n011\n111\n111\n110\n101\n100\n011\n010\n001\n000\n82\nAlgorithms\nThe slow spread of a fast algorithm\nIn 1963, during a meeting of President Kennedy\u2019s scienti\ufb01c advisors, John Tukey, a math-\nematician from Princeton, explained to IBM\u2019s Dick Garwin a fast method for computing\nFourier transforms. Garwin listened carefully, because he was at the time working on ways\nto detect nuclear explosions from seismographic data, and Fourier transforms were the bot-\ntleneck of his method. When he went back to IBM, he asked John Cooley to implement\nTukey\u2019s algorithm; they decided that a paper should be published so that the idea could not\nbe patented.\nTukey was not very keen to write a paper on the subject, so Cooley took the initiative.\nAnd this is how one of the most famous and most cited scienti\ufb01c papers was published in\n1965, co-authored by Cooley and Tukey. The reason Tukey was reluctant to publish the FFT\nwas not secretiveness or pursuit of pro\ufb01t via patents. He just felt that this was a simple\nobservation that was probably already known. This was typical of the period: back then\n(and for some time later) algorithms were considered second-class mathematical objects,\ndevoid of depth and elegance, and unworthy of serious attention.\nBut Tukey was right about one thing: it was later discovered that British engineers had\nused the FFT for hand calculations during the late 1930s. And\u2014to end this chapter with the\nsame great mathematician who started it\u2014a paper by Gauss in the early 1800s on (what\nelse?) interpolation contained essentially the same idea in it! Gauss\u2019s paper had remained a\nsecret for so long because it was protected by an old-fashioned cryptographic technique: like\nmost scienti\ufb01c papers of its era, it was written in Latin.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n83\nExercises\n2.1. Use the divide-and-conquer integer multiplication algorithm to multiply the two binary integers\n10011011 and 10111010.\n2.2. Show that for any positive integers n and any base b, there must some power of b lying in the\nrange [n, bn].\n2.3. Section 2.2 describes a method for solving recurrence relations which is based on analyzing the\nrecursion tree and deriving a formula for the work done at each level. Another (closely related)\nmethod is to expand out the recurrence a few times, until a pattern emerges. For instance, let\u2019s\nstart with the familiar T (n) = 2T (n/2) + O(n). Think of O(n) as being \u2264cn for some constant c,\nso: T (n) \u22642T (n/2) + cn. By repeatedly applying this rule, we can bound T (n) in terms of T (n/2),\nthen T (n/4), then T (n/8), and so on, at each step getting closer to the value of T (\u00b7) we do know,\nnamely T (1) = O(1).\nT (n)\n\u2264\n2T (n/2) + cn\n\u2264\n2[2T (n/4) + cn/2] + cn\n=\n4T (n/4) + 2cn\n\u2264\n4[2T (n/8) + cn/4] + 2cn\n=\n8T (n/8) + 3cn\n\u2264\n8[2T (n/16) + cn/8] + 3cn\n=\n16T (n/16) + 4cn\n.\n.\n.\nA pattern is emerging... the general term is\nT (n) \u22642kT (n/2k) + kcn.\nPlugging in k = log2 n, we get T (n) \u2264nT (1) + cn log2 n = O(n log n).\n(a) Do the same thing for the recurrence T (n) = 3T (n/2) + O(n). What is the general kth term\nin this case? And what value of k should be plugged in to get the answer?\n(b) Now try the recurrence T (n) = T (n \u22121) + O(1), a case which is not covered by the master\ntheorem. Can you solve this too?\n2.4. Suppose you are choosing between the following three algorithms:\n\u2022 Algorithm A solves problems by dividing them into \ufb01ve subproblems of half the size, recur-\nsively solving each subproblem, and then combining the solutions in linear time.\n\u2022 Algorithm B solves problems of size n by recursively solving two subproblems of size n \u22121\nand then combining the solutions in constant time.\n\u2022 Algorithm C solves problems of size n by dividing them into nine subproblems of size n/3,\nrecursively solving each subproblem, and then combining the solutions in O(n2) time.\nWhat are the running times of each of these algorithms (in big-O notation), and which would you\nchoose?\n2.5. Solve the following recurrence relations and give a \u0398 bound for each of them.\n(a) T (n) = 2T (n/3) + 1\n(b) T (n) = 5T (n/4) + n\n84\nAlgorithms\n(c) T (n) = 7T (n/7) + n\n(d) T (n) = 9T (n/3) + n2\n(e) T (n) = 8T (n/2) + n3\n(f) T (n) = 49T (n/25) + n3/2 log n\n(g) T (n) = T (n \u22121) + 2\n(h) T (n) = T (n \u22121) + nc, where c \u22651 is a constant\n(i) T (n) = T (n \u22121) + cn, where c > 1 is some constant\n(j) T (n) = 2T (n \u22121) + 1\n(k) T (n) = T (\u221an) + 1\n2.6. A linear, time-invariant system has the following impulse response:\n\u0000\u0001\n\u0002\u0003\n\u0004\u0005\n\u0006\u0007\n\b\t\n\n\u000b\n\f\r\n\u000e\u000f\n\u0010\u0011\n\u0012\u0013\n\u0014\u0015\n\u0016\u0017\n\u0018\n\u0018\n\u0019\n\u0019\n\u001a\u001b\n\u001c\n\u001c\u001d\n\u001e\u001f\n \n !\n\"#\n$\n$\n%\n%\n&'\n(\n()\n*+\n,-\n./\nt\nb(t)\nt0\n1/t0\n(a) Describe in words the effect of this system.\n(b) What is the corresponding polynomial?\n2.7. What is the sum of the nth roots of unity? What is their product if n is odd? If n is even?\n2.8. Practice with the fast Fourier transform.\n(a) What is the FFT of (1, 0, 0, 0)? What is the appropriate value of \u03c9 in this case? And of which\nsequence is (1, 0, 0, 0) the FFT?\n(b) Repeat for (1, 0, 1, \u22121).\n2.9. Practice with polynomial multiplication by FFT.\n(a) Suppose that you want to multiply the two polynomials x + 1 and x2 + 1 using the FFT.\nChoose an appropriate power of two, \ufb01nd the FFT of the two sequences, multiply the results\ncomponentwise, and compute the inverse FFT to get the \ufb01nal result.\n(b) Repeat for the pair of polynomials 1 + x + 2x2 and 2 + 3x.\n2.10. Find the unique polynomial of degree 4 that takes on values p(1) = 2, p(2) = 1, p(3) = 0, p(4) = 4,\nand p(5) = 0. Write your answer in the coef\ufb01cient representation.\n2.11. In justifying our matrix multiplication algorithm (Section 2.5), we claimed the following block-\nwise property: if X and Y are n \u00d7 n matrices, and\nX =\n\u0014A\nB\nC\nD\n\u0015\n,\nY =\n\u0014E\nF\nG\nH\n\u0015\n.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n85\nwhere A, B, C, D, E, F, G, and H are n/2 \u00d7 n/2 submatrices, then the product XY can be\nexpressed in terms of these blocks:\nXY\n=\n\u0014\nA\nB\nC\nD\n\u0015 \u0014\nE\nF\nG\nH\n\u0015\n=\n\u0014\nAE + BG\nAF + BH\nCE + DG\nCF + DH\n\u0015\nProve this property.\n2.12. How many lines, as a function of n (in \u0398(\u00b7) form), does the following program print? Write a\nrecurrence and solve it. You may assume n is a power of 2.\nfunction f(n)\nif n > 1:\nprint_line(\u2018\u2018still going\u2019\u2019)\nf(n/2)\nf(n/2)\n2.13. A binary tree is full if all of its vertices have either zero or two children. Let Bn denote the\nnumber of full binary trees with n vertices.\n(a) By drawing out all full binary trees with 3, 5, or 7 vertices, determine the exact values of\nB3, B5, and B7. Why have we left out even numbers of vertices, like B4?\n(b) For general n, derive a recurrence relation for Bn.\n(c) Show by induction that Bn is \u2126(2n).\n2.14. You are given an array of n elements, and you notice that some of the elements are duplicates;\nthat is, they appear more than once in the array. Show how to remove all duplicates from the\narray in time O(n log n).\n2.15. In our median-\ufb01nding algorithm (Section 2.4), a basic primitive is the split operation, which\ntakes as input an array S and a value v and then divides S into three sets: the elements less\nthan v, the elements equal to v, and the elements greater than v. Show how to implement this\nsplit operation in place, that is, without allocating new memory.\n2.16. You are given an in\ufb01nite array A[\u00b7] in which the \ufb01rst n cells contain integers in sorted order and\nthe rest of the cells are \ufb01lled with \u221e. You are not given the value of n. Describe an algorithm that\ntakes an integer x as input and \ufb01nds a position in the array containing x, if such a position exists,\nin O(log n) time. (If you are disturbed by the fact that the array A has in\ufb01nite length, assume\ninstead that it is of length n, but that you don\u2019t know this length, and that the implementation\nof the array data type in your programming language returns the error message \u221ewhenever\nelements A[i] with i > n are accessed.)\n2.17. Given a sorted array of distinct integers A[1, . . . , n], you want to \ufb01nd out whether there is an\nindex i for which A[i] = i. Give a divide-and-conquer algorithm that runs in time O(log n).\n2.18. Consider the task of searching a sorted array A[1 . . . n] for a given element x: a task we usually\nperform by binary search in time O(log n). Show that any algorithm that accesses the array only\nvia comparisons (that is, by asking questions of the form \u201cis A[i] \u2264z?\u201d), must take \u2126(log n) steps.\n2.19. A k-way merge operation. Suppose you have k sorted arrays, each with n elements, and you want\nto combine them into a single sorted array of kn elements.\n86\nAlgorithms\n(a) Here\u2019s one strategy: Using the merge procedure from Section 2.3, merge the \ufb01rst two ar-\nrays, then merge in the third, then merge in the fourth, and so on.\nWhat is the time\ncomplexity of this algorithm, in terms of k and n?\n(b) Give a more ef\ufb01cient solution to this problem, using divide-and-conquer.\n2.20. Show that any array of integers x[1 . . . n] can be sorted in O(n + M) time, where\nM\n=\nmax\ni\nxi \u2212min\ni\nxi.\nFor small M, this is linear time: why doesn\u2019t the \u2126(n log n) lower bound apply in this case?\n2.21. Mean and median. One of the most basic tasks in statistics is to summarize a set of observations\n{x1, x2, . . . , xn} \u2286R by a single number. Two popular choices for this summary statistic are:\n\u2022 The median, which we\u2019ll call \u00b51\n\u2022 The mean, which we\u2019ll call \u00b52\n(a) Show that the median is the value of \u00b5 that minimizes the function\nX\ni\n|xi \u2212\u00b5|.\nYou can assume for simplicity that n is odd. (Hint: Show that for any \u00b5 \u0338= \u00b51, the function\ndecreases if you move \u00b5 either slightly to the left or slightly to the right.)\n(b) Show that the mean is the value of \u00b5 that minimizes the function\nX\ni\n(xi \u2212\u00b5)2.\nOne way to do this is by calculus. Another method is to prove that for any \u00b5 \u2208R,\nX\ni\n(xi \u2212\u00b5)2 =\nX\ni\n(xi \u2212\u00b52)2 + n(\u00b5 \u2212\u00b52)2.\nNotice how the function for \u00b52 penalizes points that are far from \u00b5 much more heavily than the\nfunction for \u00b51. Thus \u00b52 tries much harder to be close to all the observations. This might sound\nlike a good thing at some level, but it is statistically undesirable because just a few outliers can\nseverely throw off the estimate of \u00b52. It is therefore sometimes said that \u00b51 is a more robust\nestimator than \u00b52. Worse than either of them, however, is \u00b5\u221e, the value of \u00b5 that minimizes the\nfunction\nmax\ni\n|xi \u2212\u00b5|.\n(c) Show that \u00b5\u221ecan be computed in O(n) time (assuming the numbers xi are small enough\nthat basic arithmetic operations on them take unit time).\n2.22. You are given two sorted lists of size m and n.\nGive an O(log m + log n) time algorithm for\ncomputing the kth smallest element in the union of the two lists.\n2.23. An array A[1 . . . n] is said to have a majority element if more than half of its entries are the\nsame. Given an array, the task is to design an ef\ufb01cient algorithm to tell whether the array has a\nmajority element, and, if so, to \ufb01nd that element. The elements of the array are not necessarily\nfrom some ordered domain like the integers, and so there can be no comparisons of the form \u201cis\nA[i] > A[j]?\u201d. (Think of the array elements as GIF \ufb01les, say.) However you can answer questions\nof the form: \u201cis A[i] = A[j]?\u201d in constant time.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n87\n(a) Show how to solve this problem in O(n log n) time. (Hint: Split the array A into two arrays\nA1 and A2 of half the size. Does knowing the majority elements of A1 and A2 help you \ufb01gure\nout the majority element of A? If so, you can use a divide-and-conquer approach.)\n(b) Can you give a linear-time algorithm? (Hint: Here\u2019s another divide-and-conquer approach:\n\u2022 Pair up the elements of A arbitrarily, to get n/2 pairs\n\u2022 Look at each pair: if the two elements are different, discard both of them; if they are\nthe same, keep just one of them\nShow that after this procedure there are at most n/2 elements left, and that they have a\nmajority element if and only if A does.)\n2.24. On page 66 there is a high-level description of the quicksort algorithm.\n(a) Write down the pseudocode for quicksort.\n(b) Show that its worst-case running time on an array of size n is \u0398(n2).\n(c) Show that its expected running time satis\ufb01es the recurrence relation\nT (n) \u2264O(n) + 1\nn\nn\u22121\nX\ni=1\n(T (i) + T (n \u2212i)).\nThen, show that the solution to this recurrence is O(n log n).\n2.25. In Section 2.1 we described an algorithm that multiplies two n-bit binary integers x and y in\ntime na, where a = log2 3. Call this procedure fastmultiply(x, y).\n(a) We want to convert the decimal integer 10n (a 1 followed by n zeros) into binary. Here is the\nalgorithm (assume n is a power of 2):\nfunction pwr2bin(n)\nif n = 1:\nreturn 10102\nelse:\nz =???\nreturn fastmultiply(z, z)\nFill in the missing details. Then give a recurrence relation for the running time of the\nalgorithm, and solve the recurrence.\n(b) Next, we want to convert any decimal integer x with n digits (where n is a power of 2) into\nbinary. The algorithm is the following:\nfunction dec2bin(x)\nif n = 1:\nreturn binary[x]\nelse:\nsplit x into two decimal numbers xL, xR with n/2 digits each\nreturn ???\nHere binary[\u00b7] is a vector that contains the binary representation of all one-digit integers.\nThat is, binary[0] = 02, binary[1] = 12, up to binary[9] = 10012. Assume that a lookup in\nbinary takes O(1) time.\nFill in the missing details. Once again, give a recurrence for the running time of the algo-\nrithm, and solve it.\n88\nAlgorithms\n2.26. Professor F. Lake tells his class that it is asymptotically faster to square an n-bit integer than to\nmultiply two n-bit integers. Should they believe him?\n2.27. The square of a matrix A is its product with itself, AA.\n(a) Show that \ufb01ve multiplications are suf\ufb01cient to compute the square of a 2 \u00d7 2 matrix.\n(b) What is wrong with the following algorithm for computing the square of an n \u00d7 n matrix?\n\u201cUse a divide-and-conquer approach as in Strassen\u2019s algorithm, except that in-\nstead of getting 7 subproblems of size n/2, we now get 5 subproblems of size n/2\nthanks to part (a). Using the same analysis as in Strassen\u2019s algorithm, we can\nconclude that the algorithm runs in time O(nlog2 5).\u201d\n(c) In fact, squaring matrices is no easier than matrix multiplication. In this part, you will\nshow that if n \u00d7 n matrices can be squared in time S(n) = O(nc), then any two n \u00d7 n\nmatrices can be multiplied in time O(nc).\ni. Given two n \u00d7 n matrices A and B, show that the matrix AB + BA can be computed in\ntime 3S(n) + O(n2).\nii. Given two n \u00d7 n matrices X and Y , de\ufb01ne the 2n \u00d7 2n matrices A and B as follows:\nA =\n\u0014 X\n0\n0\n0\n\u0015\nand B =\n\u0014 0\nY\n0\n0\n\u0015\n.\nWhat is AB + BA, in terms of X and Y ?\niii. Using (i) and (ii), argue that the product XY can be computed in time 3S(2n) + O(n2).\nConclude that matrix multiplication takes time O(nc).\n2.28. The Hadamard matrices H0, H1, H2, . . . are de\ufb01ned as follows:\n\u2022 H0 is the 1 \u00d7 1 matrix\n\u00021\u0003\n\u2022 For k > 0, Hk is the 2k \u00d7 2k matrix\nHk =\n\u0014 Hk\u22121\nHk\u22121\nHk\u22121\n\u2212Hk\u22121\n\u0015\nShow that if v is a column vector of length n = 2k, then the matrix-vector product Hkv can be\ncalculated using O(n log n) operations. Assume that all the numbers involved are small enough\nthat basic arithmetic operations like addition and multiplication take unit time.\n2.29. Suppose we want to evaluate the polynomial p(x) = a0 + a1x + a2x2 + \u00b7 \u00b7 \u00b7 + anxn at point x.\n(a) Show that the following simple routine, known as Horner\u2019s rule, does the job and leaves the\nanswer in z.\nz = an\nfor i = n \u22121 downto 0:\nz = zx + ai\n(b) How many additions and multiplications does this routine use, as a function of n? Can you\n\ufb01nd a polynomial for which an alternative method is substantially better?\n2.30. This problem illustrates how to do the Fourier Transform (FT) in modular arithmetic, for exam-\nple, modulo 7.\nS. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani\n89\n(a) There is a number \u03c9 such that all the powers \u03c9, \u03c92, . . . , \u03c96 are distinct (modulo 7). Find this\n\u03c9, and show that \u03c9 + \u03c92 + \u00b7 \u00b7 \u00b7 + \u03c96 = 0. (Interestingly, for any prime modulus there is such\na number.)\n(b) Using the matrix form of the FT, produce the transform of the sequence (0, 1, 1, 1, 5, 2) mod-\nulo 7; that is, multiply this vector by the matrix M6(\u03c9), for the value of \u03c9 you found earlier.\nIn the matrix multiplication, all calculations should be performed modulo 7.\n(c) Write down the matrix necessary to perform the inverse FT. Show that multiplying by this\nmatrix returns the original sequence. (Again all arithmetic should be performed modulo 7.)\n(d) Now show how to multiply the polynomials x2 + x + 1 and x3 + 2x \u22121 using the FT modulo\n7.\n2.31. In Section 1.2.3, we studied Euclid\u2019s algorithm for computing the greatest common divisor (gcd)\nof two positive integers: the largest integer which divides them both. Here we will look at an\nalternative algorithm based on divide-and-conquer.\n(a) Show that the following rule is true.\ngcd(a, b)\n=\n\uf8f1\n\uf8f2\n\uf8f3\n2 gcd(a/2, b/2)\nif a, b are even\ngcd(a, b/2)\nif a is odd, b is even\ngcd((a \u2212b)/2, b)\nif a, b are odd\n(b) Give an ef\ufb01cient divide-and-conquer algorithm for greatest common divisor.\n(c) How does the ef\ufb01ciency of your algorithm compare to Euclid\u2019s algorithm if a and b are n-bit\nintegers? (In particular, since n might be large you cannot assume that basic arithmetic\noperations like addition take constant time.)\n2.32. In this problem we will develop a divide-and-conquer algorithm for the following geometric task.\nCLOSEST PAIR\nInput: A set of points in the plane, {p1 = (x1, y1), p2 = (x2, y2), . . . , pn = (xn, yn)}\nOutput: The closest pair of points: that is, the pair pi \u0338= pj for which the distance\nbetween pi and pj, that is,\nq\n(xi \u2212xj)2 + (yi \u2212yj)2,\nis minimized.\nFor simplicity, assume that n is a power of two, and that all the x-coordinates xi are distinct, as\nare the y-coordinates.\nHere\u2019s a high-level overview of the algorithm:\n\u2022 Find a value x for which exactly half the points have xi < x, and half have xi > x. On this\nbasis, split the points into two groups, L and R.\n\u2022 Recursively \ufb01nd the closest pair in L and in R. Say these pairs are pL, qL \u2208L and pR, qR \u2208R,\nwith distances dL and dR respectively. Let d be the smaller of these two distances.\n\u2022 It remains to be seen whether there is a point in L and a point in R that are less than\ndistance d apart from each other. To this end, discard all points with xi < x\u2212d or xi > x+d\nand sort the remaining points by y-coordinate.\n\u2022 Now, go through this sorted list, and for each point, compute its distance to the seven sub-\nsequent points in the list. Let pM, qM be the closest pair found in this way.\n90\nAlgorithms\n\u2022 The answer is one of the three pairs {pL, qL}, {pR, qR}, {pM, qM}, whichever is closest.\n(a) In order to prove the correctness of this algorithm, start by showing the following property:\nany square of size d \u00d7 d in the plane contains at most four points of L.\n(b) Now show that the algorithm is correct. The only case which needs careful consideration is\nwhen the closest pair is split between L and R.\n(c) Write down the pseudocode for the algorithm, and show that its running time is given by\nthe recurrence:\nT (n) = 2T (n/2) + O(n log n).\nShow that the solution to this recurrence is O(n log2 n).\n(d) Can you bring the running time down to O(n log n)?\n"
}